---
title: 'BISC 307, Lab 1: Ecoclimate & Public Health'
author: "Prof. Jackie Hatala Matthes, FA 2020"
date: 'Lab: 29 October 2020'
output:
  html_document: default
  pdf_document: default
---

### Lab 1 Objectives
1. Visualize similarities and differences in Phoenix and Boston climates.
2. Use 30-year climatology to calculate daily departures from "normal" climate and assess whether Phoenix or Boston weather is more predictable. 
3. Learn to read messy, real spreadsheet data files into R.
4. Use the five tidyverse verbs - filter, arrange, select, mutate, and summarize - as functions to manipulate real data.
5. Investigate patterns in individually experienced temperatures (IETs) in Phoenix on a sub-daily timescale by neighborhood, age, and income.

### 1. Introduction: Packages, Data
In this lab, we'll learn to work with Rstudio to load and visualize a dataset describing the climates of Boston and Phoenix. Our objectives for today will build on what you read in preparation for this lab, [Sections 3.1-3.6 from the R for Data Science](http://r4ds.had.co.nz/data-visualisation.html#introduction-1) book. 

Today we will use functions from the `tidyverse` package. You should have already installed `tidyverse` during your Pre-Lab 1 R4DS exercises. After you've run the `install.packages()` once you can load that particular package with the `library()` function. You'll need to load the libraries that you'd like to use every time you restart Rstudio, which helps to save space on your computer. 

```{r message=FALSE, warning=FALSE}
# Only run this next line ONCE EVER without the # in front: 
#install.packages("tidyverse")
library(tidyverse)
library(lubridate)
```

Next we will read our data into R. In the second half of lab we'll load data from a spreadsheet csv file, but for now we'll work with pre-formatted data in R. To load the .RData file, we use the `load()` function. You can also do this step by going to  `File > Open File` on the Rstudio top menu bar, and then selecting the .Rdata file to load. However, it's better to do this by writing out the actual code, so that you can save it and all of your steps are reproducible. 

```{r}
# Load .RData file with Boston & Phoenix climate data 
load("data/BOSPHO_climate.RData")

```

This should create two objects in your Rstudio "Environment" panel (upper right box of Rstudio) called `BOS_climate` and `PHO_climate`. Both are data frame objects with  rows as observations and 5 columns with the variables. We can take a look at the first few rows of the data frame by running:

```{r}
BOS_climate
```

We can see the data frame dimensions (numbers of rows and columns) in addition to the variable (column) names and the class of the data in each variable. In this data frame, the `station` variable is a character class (letters), the `date` is a date class, `tmax` and `tmin` are integers, and `precip` is a numeric decimal class. 

Now let's use ggplot to visualize the maximum temperature in Boston over the whole time period. 

```{r}
# Plot Boston maximum temperature
ggplot(data = BOS_climate) + 
  geom_line(mapping = aes(x = date, y = tmax, col=station)) +
  labs(x = "Date", y = "Maximum Temperature (C)")

```

The syntax of ggplot and its arguments should look familiar from the R4DS reading. The first argument to ggplot identifies the data frame for plotting, and `geoms` are added that specify how the data are mapped onto the plot. For this plot, we are mapping `date` onto the x-axis, `tmax` onto the y-axis, and coloring the lines by station. We can also change the axis labels by adding the `labs` function to the `ggplot` list. 

***
**Code Challenge 1:
Within your group, create a plot that shows the daily precipitation values for Boston.**

***

### 2. Climatology

The data that we've visualized so far are daily weather data. Now we'll aggregate these data into a climatology to determine whether any one day, month, or year is different from the long-term normal value. To transform weather data into climatology, a 30-year averaging period is typically used because this period captures many years that can vary with cyclical patterns like El Niño. 

Climatological periods are defined by the World Meteorological Organization as a 30-year interval that starts with a year that ends in a 1 and end on a year that ends on a 0, so the nearest climatological period to 2019 (the last complete year) is 1981-2010. This is called a climatological "normal" because we use it to evaluate whether any particular value is different from normal conditions. 

The first step for our climatology is to clip our data to 1981-2010. To do this, we'll use the `filter()` function from the `tidyverse` package. Almost all of the `tidyverse` data processing functions have a similar structure to arguments:

1. The first argument is the data frame we're using.
2. The second argument describes how we'd like to change the data. 

Here, the second argument is using a function from the `lubridate` package that we loaded at the beginning -- `year()` -- to return just the year from the `date` variable. We're using the `year()` function to only select the years that are greater than 1980 and less than 2011. (Equivalently, we could have written `year(date) >= 1981 & year(date) <= 2010` with less than/greater than or equal for this second argument). 

```{r}
# Clip out 1981-2010 in BOS from the climate data
BOS_climrecent <- filter(BOS_climate, year(date) > 1980 & year(date) < 2011)

# Plot Phoenix and Boston maximum temperature, 1981-2010
ggplot(data = BOS_climrecent) + 
  geom_line(mapping = aes(x = date, y = tmax)) +
  labs(x = "Date", y = "Maximum Temperature (C)")

```

The next step is to take the 30-year average for maximum temperature on each day. We'll do this with a set of three functions: 

1. `mutate()` to add a column with just the month & day for each date (removing year)
2. `group_by()` to define the month-day column as the grouping variable
3. `summarize()` to calculate the 30-year statistics over the grouping variable

```{r, warning=FALSE}
# Add a new column with just month & day from date
BOS_climrecent_v2 <- mutate(BOS_climrecent,
                            month_day = format(date, "%m-%d"))

# Look at the new month_day column
BOS_climrecent_v2

# Group data frame by month_day for climatology
# (we want to average over all 30 years for max temp on Jan-01, Jan-02, etc.)
BOS_clim_grouped <- group_by(BOS_climrecent_v2, month_day)

# Calculate 30-year average tmax for each day of the year
BOS_tmax30yr <- summarize(BOS_clim_grouped, 
                          tmax_mean = mean(tmax, na.rm=TRUE))

# Plot Boston normal maximum temperature, climatology 1981-2010
ggplot(data = BOS_tmax30yr) + 
  geom_line(mapping = aes(x = as.Date(month_day, format = "%m-%d"), 
                          y = tmax_mean)) + 
  labs(x = "Date", y = "Maximum Temperature (C)") +
  scale_x_date(date_labels = "%b")
```

Whenever we are writing a set of processing steps with intermediate objects (e.g., `BOS_climrecent` and `BOS_climrecent_v2`), it's good to think about putting those steps together into neater blocks of code by using `pipes %>%` from the `maggritr` package (included in the `tidyverse`). A pipe takes the output from one line and "pipes" it to the following line without creating an intermediate object. 

For example, we can re-write the steps from above to find the 30-year Boston climatology using pipes:

```{r}
# Summarize 30-year 1981-2010 max temperature
BOS_tmaxClimatology <- BOS_climate %>%
  filter(year(date) > 1980 & year(date) < 2011) %>%
  mutate(month_day = format(date, "%m-%d")) %>%
  group_by(month_day) %>%
  summarize(tmax_mean = mean(tmax, na.rm=TRUE))

# Plot Boston climatological maximum temperature, 1981-2010
ggplot(data = BOS_tmaxClimatology) + 
  geom_line(mapping = aes(x = as.Date(month_day, format = "%m-%d"), 
                          y = tmax_mean)) + 
  labs(x = "Date", y = "Maximum Temperature (C)") +
  scale_x_date(date_labels = "%b")

```


***
**Code Challenge 2:
Repeat the steps that we used here to calculate and visualize the 30-year climatological mean daily maximum temperature for Phoenix. BEFORE YOU START CODING, outline and review the steps that you are going to take within your group.**

***

Next, we'll calculate the difference between the daily weather data of maximum temperature and the climatological 30-year mean maximum temperature to see when values differ from normal. 

The first step is to attach a column with the 30-year mean daily values onto the original daily weather data frame. We'll use the `left_join()` function to connect the 30-year climatology by matching up the day-month in both datasets. We'll learn more about `joins` in detail later on. Then, we'll use `mutate()` to make a new column that is tmax minus tmax_mean for the difference between the actual temperature and the 30-year mean temperature on that day. 

```{r}
# Join 30-year mean daily max temp to all yearly data and calculate difference
BOS_tmaxDiff <- BOS_climate %>%
  filter(year(date) > 1980 & year(date) < 2011) %>%
  mutate(month_day = format(date, "%m-%d")) %>%
  left_join(BOS_tmaxClimatology, by = "month_day") %>%
  mutate(tmax_diff = tmax - tmax_mean)

# Look at the new data frame
BOS_tmaxDiff

# Plot Boston difference from climatological maximum temperature, 1981-2010
ggplot(data = BOS_tmaxDiff) + 
  geom_line(mapping = aes(x = date, y = tmax_diff)) + 
  labs(x = "Date", y = "Temperature difference from 30-year average (C)")

```

Now let's do the same steps to calculate the Phoenix daily departures in maximum temperature from the 30-year climatology: 

```{r}
# Summarize 30-year 1981-2010 max temperature
PHO_tmaxClimatology <- PHO_climate %>%
  filter(year(date) > 1980 & year(date) < 2011) %>%
  mutate(month_day = format(date, "%m-%d")) %>%
  group_by(month_day) %>%
  summarize(tmax_mean = mean(tmax, na.rm=TRUE))

# Join 30-year mean daily max temp to all yearly data and calculate difference
PHO_tmaxDiff <- PHO_climate %>%
  filter(year(date) > 1980 & year(date) < 2011) %>%
  mutate(month_day = format(date, "%m-%d")) %>%
  left_join(PHO_tmaxClimatology, by = "month_day") %>%
  mutate(tmax_diff = tmax - tmax_mean)
```


### 3. Weather predictability

Departures from mean climate can influence ecology and can have important effects on human health. For example, in Boston our infrastructure is built to handle large amounts of snow during the winter, but the same sized snowstorm further South could bring things to a standstill for days. Similarly, heat stress is an important factor when considering climate change: if infrastructure is built for a particular temperature within a city, unexpected temperature departures can have devastating impacts for people who cannot quickly adjust temperature conditions in their homes. 

We can assess the predictability of weather within a location by comparing daily weather to the 30-year climatology. We’ll unpredictability as larger deviations from the mean climatology. We've already calculated the difference between daily maximum temperatures and their 30-year average. To visualize this difference, we can look at a probability density of the departures from the long-term average: 

```{r}
# Plot Boston difference from climatological maximum temperature, 1981-2010
ggplot(data = BOS_tmaxDiff) + 
  geom_density(mapping = aes(x = tmax_diff)) +
  labs(x = "Daily max temp difference from 30-year normal [C]", y = "Probability") 

```

We can also get values for the mean, standard deviation, and different percentiles of the climatological departures:

```{r}
mean(BOS_tmaxDiff$tmax_diff) # mean departure (basically zero)
sd(BOS_tmaxDiff$tmax_diff) # standard deviation
quantile(BOS_tmaxDiff$tmax_diff, c(0.05, 0.95)) # 5% tails of the probability values

```

The above plot and quantile calculation shows that in Boston, there about a 5% chance of having a day that is about 14-15 degrees warmer or colder than average. This is a  small but non-negligible probability. We can use the probability density for Boston maximum temperature departures as a benchmark to assess whether Phoenix's maximum temperatures are more or less predictable than Boston's. 

***
**Code Challenge 3:
Produce a plot that shows the probability density for the difference between daily maximum tempearture and the 30-year climatological mean for Phoenix. Comparing the probability densities of maximum temperature departures for Boston and Phoenix, which city is more likely to experience an "abnormally hot" day?**

***

### 4. Individually Exerienced Temperatures (IETs) 
Now we'll investigate a [dataset](https://doi.org/10.6073/pasta/cba2173a40671a8a350c772f3da3214f) of Individually Exerienced Temperatures (IETs) from the [Central Arizona-Phoenix (CAP) Long-Term Ecological Research (LTER) site](https://sustainability.asu.edu/caplter/). You can read more details about the functions that we'll use here in the sections on Data Transformation, [Sections 5.1-5.6 from the R for Data Science book](http://r4ds.had.co.nz/transform.html). 

The IETs data are stored as `.csv` files, which stands for comma-separated values. `csv` files are a plain text format for storing tabular data, such as spreadsheets, and because they are plain text (e.g. in contrast to an Excel workbook) they are more reproducible because you can open files with any computer text editor. You can also store Excel and Google sheets as `csv` files, which is the preferred storage format for reproducible research because it is more stable as versions of Excel and Google sheets softwares change.

To load a csv file into Rstudio, we'll use `read_csv()` from the `tidyverse`. R also has a base function called `read.csv()` that doesn't require the `tidyverse` package, but `read_csv()` has a few advantages here. Helpfully, `read_csv()` preserves data classes, and recognizes dates and times.

The IETs dataset has five different csv files, and you can read more about each file in `IET_metadata.txt` file included in the `data/` directory of this repository. The metadata contains important information for interpreting the data, for example the meaning of column names, units, etc. Here we will only work with two of the csv files from this dataset - the main IET dataset and the background participant survey - but you can use the other files, if you'd like, for your lab report.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# We'll use the stringr library (part of the tidyverse) to separate some of the column data that are stored as text
library(stringr)

# Read IETs csv file into an object called IETs
IETs <- read_csv("data/647_IET_measurements_50e72be656c7407bd07e316061beeb1d.csv")

# Read background survey csv file into an object called Bkgd
Bkgd <- read_csv("data/647_background_surveys_546a5d337c7a39922a4b65ced4bb29fe.csv")

# Look at the IETs and Bkgd tibbles to see what the data look like
IETs
Bkgd
```

The two data frames are linked by the `Subject ID`, where the first number in each `Subject ID` corresponds to a neighborhood, and the letter is a unique participant from that neighborhood. 

The columns in the IETs data frame are rather straightforward, but there are LOTS of columns in the Bkgd data frame that correspond to survey answers for individual questions. We'll focus on three of these columns in this lab, but you'll need to read the `IET_metadata.txt` file if you'd like to use information from additional questions. 

### 5. Visualizing differences in IETs by neighborhood and time of day 

First we'll examine the differences in IETs grouped by neighborhood. First we need to use the tidyverse Data Transformation verbs to get the data in the right format. The IETs column for `Subject ID` includes the neighborhood, but is also stuck to the participant ID within each neighborhood. If we want to group data by neighborhood, we need to use `mutate()` to create a new data frame column with just neighborhood, and while we're at it, we'll also create a new column with just participant ID. 

To separate out the neighborhood piece of the `Subject ID` from the participant ID, we'll use a function called string subset `str_sub()` from the `stringr` package. This function clips a string (a set of letters or characters) based on the numeric order of letters within that string. For example, the `Subject ID` string in the first row of the IETs tibble is `1A`, where we want to clip out the first character `1` into the new neighborhood column and the second character `A` into the new ID column. 

We'll use these `stringr` functions in this lab, but you shouldn't worry about understanding all the details: the main point of this lab is to focus on the use of the data transformation verbs, but we can't avoid cleaning things up with stringr in order to use this dataset. Remember that learning R is like learning a new language, and you can think about the `stringr` commands in this lab as an immersion experience where you might not understand every detail, but can focus on the purpose of how these functions are helping us with the key Data Transformation verbs.

```{r}
# Make a new tibble with columns for neighborhood and ID
# We need to surround Subject ID with backticks `` because it has a space in the name
# str_sub(column name, from character, to character)
IETs_nbh <- mutate(IETs, 
                   neighborhood = str_sub(`Subject ID`,1,1),
                   ID = str_sub(`Subject ID`,2,2))

# Look at the new tibble
IETs_nbh
```

Now we can use `ggplot` to visualize IETs across neighborhoods. When we make the ggplot, R will warn us that it removed values that were NA in the data, but this is okay for our purposes.

```{r}
# Plot IETs by neighborhood, across all time periods
ggplot(IETs_nbh) + 
  geom_boxplot(mapping = aes(x = neighborhood, y = temperature))

```

In this plot it sort of looks like the IETs from neighborhood 5 are lower than those from the other neighborhoods. However, this plot summarizes the data across all times of day, which could introduce varibility into the range within each neighborhood. 

To look at differences in IETs by time of day, we'll summarize the data by each time period (12am-4am, 4am-8am, etc.) as well. We'll use `mutate()` to separate the time period strings and day of the week from the `period` column of `IETs_nbh`. We'll use the function `str_split_fixed()` from the `stringr` package to separate the period strings into two columns - one for day of week (dow) and one for hours (hrs) - at the "comma and space" within each string. 

```{r}
# Make new columns for day of week (dow) and time period (hrs)
# str_split_fixed(column name, "separator character", 
#                 number of columns to expect)[,column number]
IETs_nbh_time <- mutate(IETs_nbh, 
                 dow = str_split_fixed(period, ", ", n = 2)[,1],
                 hrs = str_split_fixed(period, ", ", n = 2)[,2])

# Look at the new tibble
IETs_nbh_time

# Plot: what is not ideal about this figure?
ggplot(IETs_nbh_time) + 
  geom_boxplot(mapping = aes(x = hrs, y = temperature, color = neighborhood))

```

*Note:* In practice, instead of using `mutate()` twice to create `IETs_nbh` and then `IETs_nbh_time`, we could have done a single `mutate()` function that added all the columns at once, but I broke these apart so that we could focus on individual steps. Whenever you are writing your code, after you get something that works, it's helpful to look back through what you did to see whether you could have combined steps to be more efficient with pipes `%>%`. 

We can fix the order of the x-axis categories to be a more intuitive order (instead of alphanumeric sorted) by adding a `scale_x_discrete()` function to ggplot, setting the limits to the order of the categories that we want to display.

```{r}
# Better Plot
ggplot(IETs_nbh_time) + 
  geom_boxplot(mapping = aes(x = hrs, y = temperature, color = neighborhood)) +
  scale_x_discrete(limits = 
                     c("12am-4am", "4am-8am", "8am-12pm", "12pm-4pm",
                       "4pm-8pm", "8pm-12am"))

# New palette of colors
ggplot(IETs_nbh_time) + 
  geom_boxplot(mapping = aes(x = hrs, y = temperature, color = neighborhood)) +
  scale_x_discrete(limits = c("12am-4am", "4am-8am", "8am-12pm", "12pm-4pm", "4pm-8pm", "8pm-12am")) +
  scale_color_brewer(palette = "Set1")

```

***
**Code Challenge 4:
Add the `facet_wrap()` function to the ggplot above to make a separate sub-plot for each day of the week. Are there some days that have more or less variability?**

***

## Summarizing differences in IETs by neighborhood by time of day 

In addition to visualizing the IETs dataset, we'll calculate summary statistics to report how much higher/lower one group was than another. To do this, we'll calculate the mean and standard deviation of temperature by neighborhood, and then by neighborhood and time interval. 

We'll use the `group_by()` function to define the grouping variables to calculate summary statistics and then `summarize()` to actually calculate the statistics. When we're calculating summary stats with `summarize()` you should almost always include `na.rm=TRUE`, which will skip `NA` values (missing values) in your dataset. The default in R is to be cautious and return `NA` if you're doing a calculation with something that includes `NA` values. We'll calculate the mean and standard deviation, and include the number of values within each group (this would be the n = ## you would report to show how many replicates your statistics represent).

```{r}
# GROUP BY NEIGHBORHOOD
# Group the IETs_nbh_time tibble by neighborhood
# Calculate mean temperature & standard deviation of temperature for each neighborhood, for each time period, and count the number of non-NA values
IETs_nbh_sum <- group_by(IETs_nbh_time, neighborhood) %>%
  summarize(Tmean = mean(temperature, na.rm=TRUE),
            Tsd = sd(temperature, na.rm=TRUE),
            count = n())

# Look at all the neighborhood summary
IETs_nbh_sum

# GROUP BY NEIGHBORHOOD & TIME PERIOD
# Group the IETs_nbh_time tibble by neighborhood and hrs
# Calculate mean temperature & standard deviation of temperature for each neighborhood, for each time period
IETs_nbh_time_sum <- group_by(IETs_nbh_time, neighborhood, hrs) %>%
  summarize(Tmean = mean(temperature, na.rm=TRUE),
            Tsd = sd(temperature, na.rm=TRUE),
            count = n())

# Look at all the summary stats of neighborhood x time period
IETs_nbh_time_sum

# Arrange tibble to find the warmest time period & neighborhood
IETs_sum_sort <- arrange(IETs_nbh_time_sum, desc(Tmean))

# Look at tibble
IETs_sum_sort
```
And finally, just like we used `read_csv()` to read data into R, there is a function called `write_csv()` that will write out a csv file. This is a potentially convenient way to save a table of summary statistics.

```{r}
write_csv(IETs_nbh_time_sum, "data/IETs_nbh_time_sum.csv")
```

### 6. Visualizing differences in neighborhood age and income 

Now we'll use the responses from voluntary participant background surveys collected before the experiment. Remember that the `Bkgd` data frame has a lot of columns because the survey was long, so for these exercises, we'll focus on two sets of survey answers: a_q1 = age and a_q6 = income.

The first thing that we'll have to do with the `Bkgd` data frame is to use `mutate()` to break apart the `Subject ID` string and add columns for neighborhood and ID like we did for the IETs data frame. We'll also use the `select()` function to only include the columns for the questions that we're interested in (age and income) to make the data more manageable. We can do this in a single step by connecting the `mutate()` and `select()` functions with a pipe `%>%`. 

```{r}
# Make new columns for neighborhood and ID, select only survey answers we want
Bkgd_nbh <- mutate(Bkgd, neighborhood = substr(`Subject ID`,1,1), 
                   ID = str_sub(`Subject ID`,2,2)) %>%
  select(neighborhood, ID, a_q1, a_q6)

# Make set of age labels from IET_metadata instead of age classes 1 - 4
age_labels <- c("1" = "<35", "2" = "35-49", "3" = "50-64", "4" = "65+")

# Plot age class by neighborhood
ggplot(Bkgd_nbh) + geom_bar(mapping = aes(x = a_q1, fill=neighborhood)) + 
  scale_x_discrete("Age", limits = age_labels) +
  scale_fill_brewer(palette = "Blues", direction = -1)

# Different style of geom_bar plot
ggplot(Bkgd_nbh) + 
  geom_bar(mapping = aes(x = a_q1, fill=neighborhood), position = "fill") + 
  scale_x_discrete("Age", limits = age_labels) +
  scale_fill_brewer(palette = "Blues", direction = -1)

# Make set of income class labels from IET_metadata instead of income classes 1 - 6
income_labels <- c("1" = "<10k", "2" = "10-30k", "3" = "31-50k", "4" = "51-75k", 
                   "5" = "76-100k", "6" = "101-150k", "7" = "151-200k", "8" = "200k+")

# Plot income by neighborhood
ggplot(Bkgd_nbh) + geom_bar(mapping = aes(x = a_q6, fill=neighborhood)) + 
  scale_x_discrete("Income", limits = income_labels) + 
  scale_fill_brewer(palette = "Spectral")

```

***
**Code Challenge 5:
Create a summary table with the mean income group, mean age group, and number of observations (i.e. count) in each, by neighborhood.**

***


***
**LAB REPORT INSTRUCTIONS:**

* Identify one question that you'd like to investigate with any of the datasets from this lab. 

* As you structure your data analysis to answer your question, produce an .R script pretending that you are starting from scratch (i.e., don't assume that you have anything loaded from doing this lab exercise). The goal is to be able to hand someone your code and be able to have them re-run your analysis to see what you did and how. When you've finished writing your .R script, you should "Push" your respository back to GitHub following the [instructions here](https://docs.google.com/document/d/1VbVZXhnzJUY9gJbHiMn75hc8r8_zgJXct6ugEPH-Eh8/edit?usp=sharing).

* In addition to your .R script that you push to GitHub to submit, for the Lab 1 Report you will turn in a text .pdf document no longer than 2 single-spaced pages in the format oulined within the [Lab Report Guidelines](https://docs.google.com/document/d/1BP7JYDuru8hdvQbqYwJ1s6pUWiVkBI9hmGoX8qB2eqA/edit?usp=sharing). 

* Your Lab 1 Report document must include at least one ggplot figure and one summary statistic that you calculated with `group_by` and `summarize`, which counts toward the 2-page limit. 

***

