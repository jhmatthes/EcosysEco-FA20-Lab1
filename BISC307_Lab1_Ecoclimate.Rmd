---
title: 'BISC 307, Lab 1A: Ecoclimate & Public Health'
author: "Prof. Jackie Hatala Matthes, FA 2020"
date: 'Lab: 29 October 2020'
output:
  html_document: default
  pdf_document: default
---

### Lab 1 Objectives
1. Visualize similarities and differences in Phoenix and Boston climates.
2. Use 30-year climatology to calculate daily departures from "normal" climate.
3. Quantitatively assess whether Phoenix or Boston weather is more predictable. 

### 1. Introduction: Packages, Data
In this lab, we'll learn to work with Rstudio to load and visualize a dataset describing the climates of Boston and Phoenix. Our lab objectives for today will build on what you read in preparation for this week's lab, [Sections 3.1-3.6 from the R for Data Science](http://r4ds.had.co.nz/data-visualisation.html#introduction-1) book. 

R is an object-oriented programming language, where the "objects" can be data tables (called data frames or tibbles in R), constants, functions, vectors, matrices, or lists. We'll build up to learning some of these other types of objects through this class, but today we'll exclusively focus on data frames because this is probably the most common type of object encountered in ecological data analysis. 

Today we will use functions from the `tidyverse` package, which contains sets of functions that facilitate data analysis and visualization. After you've run the `install.packages()` once (this is already done for you if you're using the server version of R) you can load that particular package with the `library()` function. You'll need to load the libraries that you'd like to use every time you restart Rstudio. This seems like a pain, but is actually useful because you can end up with dozens of R libraries and keeping only a few loaded at a time can help to save space. 

```{r message=FALSE, warning=FALSE}
# Only run this next line ONCE without the # in front: 
#install.packages("tidyverse")
library(tidyverse)
library(lubridate)
```

The next thing that we need to do is load our data into R. In the second half of this lab we'll learn to load data from an external csv file (which is how many data are stored in practice), but for now we'll work with data that has been pre-formatted in R. To load this .RData file, we use the `load()` function. You can also do this step by going to  `File > Open File` on the Rstudio top menu bar, and then selecting the .Rdata file to load, but it's better to do it by writing out the actual code, so that you can save it and all of your steps are reproducible. 

```{r}
# Load .RData file with Boston & Phoenix climate data 
load("data/BOSPHO_climate.RData")

```

When you load the .RData file, this should create an object in your Rstudio "Environment" panel (in the upper right box of Rstudio software) called `BOSPHO_climate`. `BOSPHO_climate` is a data frame object with 54,962 rows (observations) and 5 columns (variables). We can take a look at the data frame summary by running:

```{r}
BOS_climate
```

From this summary output, we can see that the dimensions (numbers of rows and columns of the data) in addition to the variable (column) names and the class of the data in each variable. In this data frame, the `station` variable is a character class (letters), the `date` is a date class, `tmax` and `tmin` are integers, and `precip` is a numeric decimal value. 

Now let's make a ggplot of the maximum temperature in Boston over the whole time period. 

```{r}
# Plot Boston maximum temperature
ggplot(data = BOS_climate) + 
  geom_line(mapping = aes(x = date, y = tmax, col=station)) +
  labs(x = "Date", y = "Maximum Temperature [C]")

```

The syntax of the ggplot function and arguments should look familiar from the R4DS reading. As a reminder, the first argument to ggplot identifies the data frame for plotting, and then `geoms` are added that specify how the data are mapped onto the plot. For this plot, we are mapping `date` onto the x-axis, `tmax` onto the y-axis, and coloring the lines by station. We can also change the axis labels by adding the `labs` function to the `ggplot` list. 

***
**Code Challenge 1:
Create a plot that shows the daily precipitation values for the whole time series for Boston and a second precipitation plot for Phoenix.**

***

### 2. Climatology

The data that we've visualized so far are weather data (daily values). Now let's aggregate these data into a climatology that will allow us to determine whether any one day, month, or year is different from the long-term normal value for that area. For transforming weather data into climatology, a 30-year averaging period is typically used because this period captures many high/low years that can vary with weather patterns like El Niño/La Niña. 

Climatological periods are defined by the World Meteorological Organization as a 30-year interval that starts with a year that ends in a 1 and end on a year that ends on a 0, so the nearest climatological period to 2019 (the last complete year) is 1981-2010. This is also called a climatological "normal" period because we use it to compare whether any paricular date/year is hotter/colder/wetter/drier than normal. The first step for our climatology analysis is to clip our data to 1981-2010. To do this, we'll use the `filter()` function from the `tidyverse` package. 

For almost all of the "data processing" functions in the tidyverse packages, the first argument is the data frame that we'd like to work with. The second argument describes how we'd like to clip the data. Here, the second argument is using another function -- `year()` -- to return just the year from the `date` variable. We're using the `year()` function to only select the years that are greater than 1980 and less than 2011. (Equivalently, we could have written `year(date) >= 1981 & year(date) <= 2010` with less than/greater than or equal for this second argument). 

```{r}
# Clip out 1981-2010 in BOS from the climate data
BOS_climrecent <- filter(BOS_climate, year(date) > 1980 & year(date) < 2011)

# Plot Phoenix and Boston maximum temperature, 1981-2010
ggplot(data = BOS_climrecent) + 
  geom_line(mapping = aes(x = date, y = tmax))

```

The next step is to take the 30-year average for maximum temperature on each day, so that we can determine whether any particular day in a year is hotter/colder than "normal". We'll do this with a set of three functions: 

1. `mutate()` to add a column with just the month & day for each date (removing year)
2. `group_by()` to define the month-day column as the grouping variable
3. `summarize()` to calculate the 30-year statistics over the grouping variable

```{r, warning=FALSE}
# Add a new tibble column with just month & day from date
BOS_climrecent_v2 <- mutate(BOS_climrecent,
                            month_day = format(date, "%m-%d"))

# Group tibble by month.day for climatology
BOS_clim_grouped <- group_by(BOS_climrecent_v2, month_day)

# Calculate 30-year average tmax for each day of the year
BOS_tmax30yr <- summarize(BOS_clim_grouped, 
                          tmax_mean = mean(tmax, na.rm=TRUE))

# Plot Boston climatological maximum temperature, 1981-2010
ggplot(data = BOS_tmax30yr) + 
  geom_line(mapping = aes(x = as.Date(month_day, format = "%m-%d"), 
                          y = tmax_mean)) + 
  labs(x = "Date", y = "Maximum Temperature [C]") +
  scale_x_date(date_labels = "%b")

```

Whenever we are writing a set of data processing steps with intermediate objects (e.g., `BOS_climrecent` and `BOS_climrecent_v2`), it's good to think about putting those steps together into neater and more efficient blocks of code by using `pipes %>%` from the `maggritr` package (included in the `tidyverse`). A pipe takes the output from that line and "pipes" it to the following line. For example, if we re-write the steps that we took above to find the 30-year Boston climatology using pipes, we'd have:

```{r}
# Summarize 30-year 1981-2010 max temperature
BOS_tmaxClimatology <- BOS_climate %>%
  filter(year(date) > 1980 & year(date) < 2011) %>%
  mutate(month_day = format(date, "%m-%d")) %>%
  group_by(month_day) %>%
  summarize(tmax_mean = mean(tmax, na.rm=TRUE))

# Plot Boston climatological maximum temperature, 1981-2010
ggplot(data = BOS_tmaxClimatology) + 
  geom_line(mapping = aes(x = as.Date(month_day, format = "%m-%d"), 
                          y = tmax_mean)) + 
  labs(x = "Date", y = "Maximum Temperature [C]") +
  scale_x_date(date_labels = "%b")

```


***
**Code Challenge 2:
Repeat the steps that we used here to calculate and visualize the 30-year climatological mean daily maximum temperature for Phoenix. BEFORE YOU START CODING, outline and review with your partner the steps that you are going to take.**

***

Next, we'll calculate the difference between the weather data values of daily maximum temperature and the climatological 30-year mean maximum temperature. The first step to doing this is to attach a column with the 30-year mean values for each date onto the original climate data frame. We'll use the `left_join()` function to connect the 30-year climatology by matching up the day-month in both datasets. We'll learn more about `joins` in detail later on in this class. Then, we'll use `mutate()` to make a new column that is tmax - tmax_mean to represent the difference between the actual temperature and the 30-year mean temperature on that day. 

```{r}
# Join 30-year mean daily max temp to all yearly data and calculate difference
BOS_climatology <- BOS_climate %>%
  filter(year(date) > 1980 & year(date) < 2011) %>%
  mutate(month_day = format(date, "%m-%d")) %>%
  left_join(BOS_tmaxClimatology, by = "month_day") %>%
  mutate(tmax_diff = tmax - tmax_mean)

# Plot Boston difference from climatological maximum temperature, 1981-2010
ggplot(data = BOS_climatology) + 
  geom_line(mapping = aes(x = date, y = tmax_diff)) + 
  labs(x = "Date", y = "Temperature difference from 30-year average [C]")

```

Now let's do the same steps to calculate the Phoenix daily departures in maximum temperature from the 30-year climatology: 

```{r}
# Summarize 30-year 1981-2010 max temperature
PHO_tmaxClimatology <- PHO_climate %>%
  filter(year(date) > 1980 & year(date) < 2011) %>%
  mutate(month_day = format(date, "%m-%d")) %>%
  group_by(month_day) %>%
  summarize(tmax_mean = mean(tmax, na.rm=TRUE))

# Join 30-year mean daily max temp to all yearly data and calculate difference
PHO_climatology <- PHO_climate %>%
  filter(year(date) > 1980 & year(date) < 2011) %>%
  mutate(month_day = format(date, "%m-%d")) %>%
  left_join(PHO_tmaxClimatology, by = "month_day") %>%
  mutate(tmax_diff = tmax - tmax_mean)

```


### 3. Weather predictability

Departures from the mean climate can influence ecological interactions and can have important effects on human health. For example, in Boston our infrastructure is built to handle large amounts of snow during the winter, but the same sized, but unexpected, snowstorm further South could bring things to a standstill for days. Similarly, heat stress is an important factor when considering climate change: if infrustructure is acclimated to a particular temperature within a city, unexpected heat events can have devastating consequences for people without refuge from the extreme heat. 

We can assess predictability of weather within a particular location by comparing daily weather patterns against the 30-year climatology. We’ll define the weather as being more unpredictable when it deviates more from the long-term data. We've already done the groundwork to assess predictability, through calculating the difference between daily maximum temperature and the 30-year average. To visualize this difference, we can look at a histogram (probability density) of the departures from the long-term average: 

```{r}
# Plot Boston difference from climatological maximum temperature, 1981-2010
ggplot(data = BOS_climatology) + 
  geom_density(mapping = aes(x = tmax_diff)) +
  labs(x = "Daily max temp difference from 30-year normal [C]", y = "Probability") 

```

This plot shows that in Boston, there about a 0.4% chance of having a day that is 20 degrees warmer or colder than average. This is a rather small, but non-negligible, probability. We can use the probability density for Boston maximum temperature departures as a benchmark to assess whether Phoenix's maximum temperatures are more or less predictable than Boston's. 

***
**Code Challenge 3:
Produce a plot that shows the probability density for the difference between daily maximum tempearture and the 30-year climatological mean for Phoenix. From comparing the probability densities of maximum temperature departures for Boston and Phoenix, which city is more likely to experience an abnormally hot day?**

***

### 4. Individually Exerienced Temperatures (IETs) & reading .csv files into R
Now we'll pivot to investgiating a dataset of Individually Exerienced Temperatures (IETs) from the [Central Arizona-Phoenix (CAP) Long-Term Ecological Research (LTER) site](https://sustainability.asu.edu/caplter/). You can read more details about the functions that we'll use here in the sections on Data Transformation, [Sections 5.1-5.6 from the R for Data Science book](http://r4ds.had.co.nz/transform.html). 

The IETs data are stored as `.csv` files, which stands for comma-separated values. `csv` files are a plain text format for storing tabular data, such as spreadsheets, and because they are plain text (e.g. in contrast to an Excel workbook) they are smaller, much more portable across computer platforms, and generally easier to work with. If you open a `csv` file with a plain text editor, like WordPad on Windows or textEdit on a Mac, you'll see rows of data where each column is separated by a comma. You can also open `csv` files with Excel or Google sheets, where they are usually automatically converted into separate columns. Importantly, you can also store Excel and Google sheets as `csv` files, which is the generally preferred long-term storage format for reproducible research because it is plain text and more stable as versions of Excel and Google sheets softwares change.

To load a csv file into Rstudio, we'll use the `read_csv()` function from the `tidyverse`. R also has a base function called `read.csv()` that doesn't require the `tidyverse` package, but `read_csv()` has a few advantages over the base function. Helpfully, `read_csv()` preserves data classes, recognizes dates and times, and automatically loads data as tibbles (rather than unformatted data frames).

The IETs dataset is comprised of five csv files, and you can read more about each file in the metadata within the `IET_metadata.txt` file included in the `data/` directory of the .zip folder for this week. The metadata contains important information that is necessary for interpreting the data, for example the meaning of column names, units for numeric values, etc. These exercises will only work with two of the csv files from this dataset - the main IET dataset and the background participant survey - but you can use the other files, if you'd like, for your independent investigation for your lab assignment. For now, let's load the two csv files that we'll be working with.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# We'll use the stringr library to separate some of the column data
library(stringr)

# Read IETs csv file into tibble called `IETs`
IETs <- read_csv("data/647_IET_measurements_50e72be656c7407bd07e316061beeb1d.csv")

# Read background survey csv file into tibble called `Bkgd`
Bkgd <- read_csv("data/647_background_surveys_546a5d337c7a39922a4b65ced4bb29fe.csv")

# Look at the IETs and Bkgd tibbles to see what the data look like
IETs
Bkgd
```

The two data frames are linked by the `Subject ID`, and if you read the `IET_metadata.txt` file, you'll learn that the first number in each `Subject ID` corresponds to a neighborhood, and the letter corresponds to each unique study participant from that neighborhood. The columns in the IETs data frame are rather straightforward to interpret, but there are LOTS of data columns in the Bkgd data frame that correspond to survey answers for individual questions. We'll focus on three of these later on in this lab, but you'll need to read the metadata if you'd like to use information from additional questions.  

### 5. Visualizing differences in IETs by neighborhood and time of day 

First we'll examine the differences in IETs grouped by neighborhood. We need to use the tidyverse Data Transformation verbs to get the data in the right format. The IETs column for `Subject ID` includes the neighborhood, but is also stuck to the participant ID within each neighborhood. If we want to group data by neighborhood, we need to use `mutate()` to create a new data frame column with JUST neighborhood, and while we're at it, we'll also create a new column with just participant ID. 

To separate out the neighborhood piece of the `Subject ID` from the participant ID, we'll use a function called string subset - `str_sub()` from the `stringr` tidyverse package - that clips a string (set of letters or characters) based on the numeric order of letters within that string. For example, the `Subject ID` string in the first row of the IETs tibble is `1A`, where we want to clip out the first character `1` into the new neighborhood column and the second character `A` into the new ID column. 

We'll use these `stringr` functions in this lab, but you shouldn't worry about understanding all the details: the main point of this lab is to focus on the use of the data transformation verbs, but we can't avoid cleaning things up with stringr in order to use this dataset. Remember that learning R is like learning a new language, and you can think about the stringr commands in this lab as an immersion experience where you might not understand every detail, but can focus on the purpose of how these functions are helping us with the key Data Transformation verbs.

```{r}
# Make a new tibble with columns for neighborhood and ID
# We need to surround Subject ID with backticks `` becuase it has a space in the name
IETs_nbh <- mutate(IETs, neighborhood = str_sub(`Subject ID`,1,1), 
                 ID = str_sub(`Subject ID`,2,2))

# Look at the new tibble
IETs_nbh
```

We can then use `ggplot` to visualize the difference in IETs across neighborhoods. When we make the ggplot, R will warn us that it removed values that were NA in the data, but this is okay for our purposes and you can ignore it.

```{r}
# Plot IETs by neighborhood, across all time periods
ggplot(IETs_nbh) + 
  geom_boxplot(mapping = aes(x = neighborhood, y = temperature))

```

This plot is interesting and it sort of looks like the IETs from neighborhood 5 are lower than those from the other neighborhoods. However, this plot summarizes the data across all times of day, which might be introducing much more varibility into the range within each neighborhood. 

To get a closer look at differences in IETs by time of day, let's summarize the data within each time period (12am-4am, 4am-8am, etc.) as well. We'll use `mutate()` to grab the time period strings from the `period` column of the IETs_nbh dataset, since they're also attached to the day of the week. We'll use `mutate()` with the function `str_split_fixed()` from the `stringr` package to separate the period strings into two columns - one for day of week (dow) and one for hours (hrs) - at the "comma and space" within each string. 

```{r}
# Make new columns for day of week (dow), time period (hrs), and am/pm (ampm)
IETs_nbh_time <- mutate(IETs_nbh, 
                 dow = str_split_fixed(period, ", ", n = 2)[,1],
                 hrs = str_split_fixed(period, ", ", n = 2)[,2])

# Look at the new tibble
IETs_nbh_time

# Plot: what is not ideal about this figure?
ggplot(IETs_nbh_time) + 
  geom_boxplot(mapping = aes(x = hrs, y = temperature, color = neighborhood))

```

*Note:* In practice, instead of using `mutate()` twice to create `IETs_nbh` and then `IETs_nbh_time`, we could have done a single `mutate()` function that added all the columns at once, but I broke these apart so that we could focus on individual problems. Whenever you are writing your code, after you get something that works, it's helpful to look back through what you did to see whether you could have combined steps to be more efficient.

We can fix the order of the x-axis categories to be a more intuitive order (instead of alphanumeric sorted) by adding a `scale_x_discrete()` function to ggplot, setting the limits to the order of the categories that we want to display.

```{r}
# Better Plot
ggplot(IETs_nbh_time) + 
  geom_boxplot(mapping = aes(x = hrs, y = temperature, color = neighborhood)) +
  scale_x_discrete(limits = 
                     c("12am-4am", "4am-8am", "8am-12pm", "12pm-4pm",
                       "4pm-8pm", "8pm-12am"))

# New palette of colors
ggplot(IETs_nbh_time) + 
  geom_boxplot(mapping = aes(x = hrs, y = temperature, color = neighborhood)) +
  scale_x_discrete(limits = c("12am-4am", "4am-8am", "8am-12pm", "12pm-4pm", "4pm-8pm", "8pm-12am")) +
  scale_color_brewer(palette = "Set1")

```

***
**Code Challenge 4:
Add the `facet_wrap()` function to the ggplot above to make a separate sub-plot for each day of the week. Are there some days that have much more or much less variability?**

***

## Summarizing differences in IETs by neighborhood by time of day 

In addition to visualizing the IETs dataset, we'll also want to calculate summary statistics to report how much one group was higher/lower than another. To do this, we'll calculate summary statistics for the mean and standard deviation of temperature by neighborhood, and then by neighborhood and time interval. To do this we'll use the `group_by()` function to set the levels at which we'd like to calculate summary statistics and then `summarize()` to actually calculate the statistics. When we're calculating summary stats with `summarize()` you should almost always include `na.rm=TRUE`, which will skip `NA` values in your dataset because the default in R is to return `NA` if it is trying to do a calculation with something that includes `NA` values. We'll calculate the mean and standard deviation, and also include the number of values within each group (this would be the n = ## you would report to show how many replicates your statistics represent).

```{r}
# GROUP BY NEIGHBORHOOD
# Group the IETs_nbh_time tibble by neighborhood 
IETs_nbh_grp <- group_by(IETs_nbh_time, neighborhood)

# Calculate mean temperature & standard deviation of temperature for each neighborhood, for each time period
IETs_nbh_sum <- summarize(IETs_nbh_grp, Tmean = mean(temperature, na.rm=TRUE),
                          Tsd = sd(temperature, na.rm=TRUE),
                          count = n())

# Look at all the neighborhood summary
IETs_nbh_sum

# GROUP BY NEIGHBORHOOD & TIME PERIOD
# Group the IETs_nbh_time tibble by neighborhood and hrs
IETs_nbh_time_grp <- group_by(IETs_nbh_time, neighborhood, hrs)

# Calculate mean temperature & standard deviation of temperature for each neighborhood, for each time period
IETs_nbh_time_sum <- summarize(IETs_nbh_time_grp, Tmean = mean(temperature, na.rm=TRUE),
                               Tsd = sd(temperature, na.rm=TRUE),
                               count = n())

# Look at all the summary stats of neighborhood x time period
print(IETs_nbh_time_sum, n = 30)

# Arrange tibble to find the warmest time period & neighborhood
IETs_sum_sort <- arrange(IETs_nbh_time_sum, desc(Tmean))

# Look at tibble
IETs_sum_sort

```

